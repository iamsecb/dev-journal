{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About","text":"<p>Daily record of musings, learnings, things to learn, gaps in knowledge and mistakes that lead to more learning \ud83d\udcd8\ud83e\udd14\ud83e\udd2f</p>"},{"location":"roadmap.mm/","title":"Roadmap","text":"<p>This is a collection of resources to skill up in Cloud and Backend Engineering.</p> <p>Note</p> <p>This list is a work in progress.</p>  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"},{"location":"roadmap.mm/#aws","title":"AWS","text":""},{"location":"roadmap.mm/#s3","title":"S3","text":"<ul> <li>AWS: Deploy a Static Website to Amazon S3 <code>workshop:manning</code></li> </ul>"},{"location":"roadmap.mm/#vpc","title":"VPC","text":""},{"location":"roadmap.mm/#databases","title":"Databases","text":"<ul> <li>AWS: Store On-Prem Data on DynamoDB <code>workshop;manning</code></li> </ul>"},{"location":"roadmap.mm/#serverless","title":"Serverless","text":"<ul> <li>AWS: Migrate an Application to Serverless AWS <code>workshop:manning</code></li> </ul>"},{"location":"roadmap.mm/#ec2","title":"EC2","text":""},{"location":"roadmap.mm/#cloudfront","title":"Cloudfront","text":""},{"location":"roadmap.mm/#iam","title":"IAM","text":"<ul> <li>AWS: AWS Security Controls <code>workshop:manning</code></li> </ul>"},{"location":"roadmap.mm/#r53","title":"R53","text":""},{"location":"roadmap.mm/#k8s","title":"K8s","text":"<ul> <li>Core Kubernetes <code>book</code></li> <li> <p>GitOps and Kubernetes <code>book</code></p> </li> <li> <p>EKS</p> <ul> <li>AWS EKS Kubernetes-Masterclass <code>course:udemy</code></li> <li>EKS immersion day <code>course:aws</code></li> </ul> </li> <li> <p>Containers</p> <ul> <li>Learning Containers, Kubernetes, and Backend Development <code>blog</code></li> </ul> </li> </ul>"},{"location":"roadmap.mm/#authentication-authorization","title":"Authentication &amp; Authorization","text":"<ul> <li>External APIs: GitHub Sign-In Application <code>workshop</code></li> </ul>"},{"location":"roadmap.mm/#networking","title":"Networking","text":""},{"location":"roadmap.mm/#security","title":"Security","text":"<ul> <li>Securing DevOps <code>book</code></li> </ul>"},{"location":"roadmap.mm/#distributed-systems","title":"Distributed systems","text":"<ul> <li> <p>System design</p> <ul> <li>Build an Orchestrator in Go <code>book</code></li> <li>Distributed Services with Go <code>book</code></li> <li>Distributed systems for fun and profit <code>book</code></li> <li>High Scaleability <code>blog</code></li> <li>needcode.io</li> </ul> </li> <li> <p>Service mesh</p> <ul> <li>Istio in Action <code>book</code></li> </ul> </li> </ul>"},{"location":"roadmap.mm/#observability","title":"Observability","text":"<ul> <li>Cloud Observability in Action <code>book</code></li> </ul>"},{"location":"roadmap.mm/#software-architecture","title":"Software Architecture","text":"<ul> <li> <p>System design</p> <ul> <li>Rocking System Design <code>course:udemy</code></li> </ul> </li> <li> <p>Design patterns</p> <ul> <li>Grokking Simplicity <code>book</code></li> </ul> </li> <li> <p>Mindset &amp; skill </p> <ul> <li>The Creative Programmer <code>book</code></li> <li>The Programmer's Brain <code>book</code></li> </ul> </li> <li> <p>Go</p> <ul> <li>Effective Go <code>book</code></li> <li>Learn Concurrent Programming with Go <code>book</code></li> <li>100 Go Mistakes and How to Avoid Them <code>book</code></li> <li>Let's Go further <code>book</code></li> </ul> </li> <li> <p>Algorithms</p> <ul> <li>Grokking Algorithms <code>book</code></li> <li>A Common-Sense Guide to Data Structures and Algorithms <code>book</code></li> <li>neetcode.io</li> </ul> </li> <li> <p>External APIs: GitHub Data Management <code>workshop</code></p> </li> </ul>"},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#tag:alb","title":"alb","text":"<ul> <li>            AWS WAF ALB Mind Notes          </li> </ul>"},{"location":"tags/#tag:api-design","title":"api-design","text":"<ul> <li>            API Design          </li> <li>            Adapter Pattern          </li> <li>            Go Reusing HTTP Client          </li> </ul>"},{"location":"tags/#tag:aws","title":"aws","text":"<ul> <li>            AWS FAQ          </li> <li>            Vault AWS Auth Method Security Advisory (HCSEC-2025-30)          </li> </ul>"},{"location":"tags/#tag:bash","title":"bash","text":"<ul> <li>            Bash Primer          </li> </ul>"},{"location":"tags/#tag:cilium","title":"cilium","text":"<ul> <li>            ALB and Cilium          </li> </ul>"},{"location":"tags/#tag:cillium","title":"cillium","text":"<ul> <li>            Cilium          </li> </ul>"},{"location":"tags/#tag:client","title":"client","text":"<ul> <li>            Go Context Timeout          </li> <li>            Go Reusing HTTP Client          </li> </ul>"},{"location":"tags/#tag:cloudwatch","title":"cloudwatch","text":"<ul> <li>            AWS CloudWatch          </li> </ul>"},{"location":"tags/#tag:context","title":"context","text":"<ul> <li>            Go Context Timeout          </li> </ul>"},{"location":"tags/#tag:daemonsets","title":"daemonsets","text":"<ul> <li>            Scheduling DaemonSets          </li> </ul>"},{"location":"tags/#tag:database","title":"database","text":"<ul> <li>            Database Index and Constraints          </li> </ul>"},{"location":"tags/#tag:dont-repeat","title":"dont-repeat","text":"<ul> <li>            17 04 2023          </li> <li>            21 04 2023          </li> </ul>"},{"location":"tags/#tag:faq","title":"faq","text":"<ul> <li>            SSL Certs          </li> </ul>"},{"location":"tags/#tag:git","title":"git","text":"<ul> <li>            19 07 2023          </li> <li>            Multiple Git Profiles          </li> </ul>"},{"location":"tags/#tag:go","title":"go","text":"<ul> <li>            Adapter Pattern          </li> <li>            Go Context Timeout          </li> <li>            Go Reusing HTTP Client          </li> </ul>"},{"location":"tags/#tag:hashicorp","title":"hashicorp","text":"<ul> <li>            Hashicorp Vault Keys for Autounseal          </li> </ul>"},{"location":"tags/#tag:istio","title":"istio","text":"<ul> <li>            Istio Egress TLS Origination          </li> </ul>"},{"location":"tags/#tag:k8s","title":"k8s","text":"<ul> <li>            Cilium          </li> <li>            Scheduling DaemonSets          </li> <li>            Vault K8s Auth          </li> </ul>"},{"location":"tags/#tag:linux","title":"linux","text":"<ul> <li>            Linux Primer          </li> </ul>"},{"location":"tags/#tag:network-policy","title":"network-policy","text":"<ul> <li>            Cilium          </li> </ul>"},{"location":"tags/#tag:networking","title":"networking","text":"<ul> <li>            Linux Primer          </li> </ul>"},{"location":"tags/#tag:ssl","title":"ssl","text":"<ul> <li>            SSL Certs          </li> <li>            TLS Troubleshooting          </li> </ul>"},{"location":"tags/#tag:terraform","title":"terraform","text":"<ul> <li>            Q&amp;A          </li> </ul>"},{"location":"tags/#tag:tls","title":"tls","text":"<ul> <li>            Istio Egress TLS Origination          </li> <li>            TLS Troubleshooting          </li> </ul>"},{"location":"tags/#tag:vault","title":"vault","text":"<ul> <li>            Hashicorp Vault Keys for Autounseal          </li> <li>            Vault 412 Index Not Present          </li> <li>            Vault AWS Auth Method Security Advisory (HCSEC-2025-30)          </li> <li>            Vault K8s Auth          </li> </ul>"},{"location":"tags/#tag:waf","title":"waf","text":"<ul> <li>            AWS WAF ALB Mind Notes          </li> </ul>"},{"location":"insights/2023/05-05-2023/adapter-pattern/","title":"Adapter Pattern","text":"<p>An interesting article that describes the need for the adapter pattern.</p> <p>https://bitfieldconsulting.com/golang/adapter</p> <p>Some takeways:</p> <ul> <li> <p>As the article states \"Dependency expertise and business logic don\u2019t mix\" because it creates 2 problems; impacts   testability and violates the single responsibility principle by breaching the scope of interest for that particular function. In the example, this is represented by mixing implementation details of a database in a function that deals with business logic.</p> </li> <li> <p>An adapter abstracts the implementation details away from the business logic. </p> <p>In the above example, defining an interface solves the issue.This would allow us to completely decouple the need for a real endpoint.</p> <pre><code>type Store interface {\n    Store(Widget) (string, error)\n}\n</code></pre> </li> <li> <p>Test inputs and outputs.  </p> <p>With an adapter, you want to ensure the data supplied as input will produce a valid result. On the same note, the return value or the output is in the format you expect it to be in.  This can be difficult to test without running a real system (e.g: database). One solution to bridge this gap is to use a mocking library. Even this may have limitation as you are still mocking the data and the real data might be different. However, you eventually will need to perform an integration test with a real system. If if it succeeds, you have shifted-left by having higher confidence that your tests will make the program work as expected. If it fails, it helps to adjust your assumption about how you thought it should work and fix the tests accordingly.</p> </li> </ul>","tags":["api-design","go"]},{"location":"insights/2023/06-05-2023/api-design/","title":"API Design","text":"","tags":["api-design"]},{"location":"insights/2023/06-05-2023/api-design/#designing-a-simple-api-client","title":"Designing a simple API client","text":"<p>A weather client as per https://bitfieldconsulting.com/golang/api-client</p> <p>Some takeaways:</p> <ul> <li>Determine what to test by inputs and outputs.</li> <li>Put any mock data for tests in <code>testdata</code>.</li> <li>when dealing API responses, you must guard against results that are unexpectedly empty, incomplete, invalid, or has the wrong schema. </li> <li> <p>A quote worth remembering:</p> <p>Don\u2019t test other people\u2019s code: test that your code does the right thing when theirs doesn\u2019t.</p> </li> <li> <p>Avoid \"paperwork\". For example, why get something from one function just to pass it to another? The API should be   simple for a developer to consume.</p> </li> </ul>","tags":["api-design"]},{"location":"insights/2023/10-08-2023/database-index-constraints/","title":"Database Index and Constraints","text":"<p>While working on a project, I came across the need to know how constraints and indexes are related to each other. Here are a few key things to know.</p> <p>The COTS app I was upgrading was a patch version upgrade and required a database migration to be perfomed. Given that this was a patch version upgrade, I did not expect any schema changes. When I looked into the fixed issues they indicated:</p> <p>The index can be renamed (usually can happen when backup and restore tools by users that deliberately rename the index)</p> <p>When  attempting to delete the original index while doing a migration, it (silently) fails and the constraints still exist in database</p> <p>Some migrations then fail because they don\u2019t expect this index to be present</p> <p>So based on this a few questions come to mind:</p>","tags":["database"]},{"location":"insights/2023/10-08-2023/database-index-constraints/#why-does-renaming-an-index-not-also-update-the-constraints-to-point-to-the-new-index-name","title":"Why does renaming an index not also update the constraints to point to the new index name?","text":"<p>The reason is that indexes and constraints are separate database objects that are related but independent of each other.</p> <p>When an index is renamed, the name change only applies to the index itself. Any constraints that reference the original index name are not automatically updated to point to the new name. This is by design, as renaming an index should not cause implicit/silent schema changes to constraints.</p> <p>Backup tools that rename indexes are likely doing so for identification/labeling purposes during the backup/restore process. But the database itself treats indexes and constraints as separate objects that reference each other by name. So a rename on the index side does not propagate or cascade an update to constraints that reference it.</p> <p>This can subsequently cause issues, as was seen in this case, if migrations expect the original index name but get the renamed one instead. The constraints are still pointing to the old name.</p>","tags":["database"]},{"location":"insights/2023/10-08-2023/database-index-constraints/#what-is-the-relationship-between-an-index-and-contrainsts","title":"What is the relationship between an Index and Contrainsts?","text":"<p>Before discussing the relationship between indexes and constraints, it's important to understand what each one is:</p> <p>Constrainsts are rules placed on the data to enforce integrity. Constraints are applied at the column or table level and prevent invalid data from being inserted or updated. Common constraints include primary keys, foreign keys, not null, unique, check constraints etc.</p> <p>An Index is defined as a way to quickly locate and access records in a database table. Indexes are data structures that contain keys built from one or more columns in a table, along with pointers to the actual row locations. Indexes are used to speed up queries and sorts on columns they are built for.</p> <p>Now that there is a foundational understanding of each, here is the relationship between indexes and constraints:</p> <p>Foreign key and unique constraints often utilize indexes internally to efficiently validate the data. When a foreign key or unique constraint is created, the database will automatically create a matching index if one does not already exist. This index supports the validation performed by the constraint. </p> <p>Let's consider a scenario where you have two tables, \"Orders\" and \"Customers.\" The \"Orders\" table has a foreign key referencing the \"Customers\" table to maintain the relationship between orders and their corresponding customers. To optimize lookup operations based on customer IDs, an index is created on the foreign key column in the \"Orders\" table. </p> <p>If the index is renamed, the foreign key constraint in the \"Orders\" table will still reference the original index name. So if you have a database migration that expects to delete original index and remove references to the index in any constraints, it will fail or produce unexpected results since the constraint is still pointing to the old index name. The constraint and index remain out of sync after the index rename.</p>","tags":["database"]},{"location":"insights/2023/17-04-2023/multi-git-profiles/","title":"Multiple Git Profiles","text":"<p>Needed to setup multiple git profiles e.g: work and personal. Discovered that there is a <code>includeIf</code> section that can be used in the <code>.gitconfig</code> file to split profile spcific information like <code>user</code>.</p> <ul> <li>Reference </li> <li>Example</li> </ul>","tags":["git"]},{"location":"insights/2023/18-05-2023/linux-primer/","title":"Linux Primer","text":"","tags":["networking","linux"]},{"location":"insights/2023/18-05-2023/linux-primer/#what-is-the-linux-kernel","title":"What is the linux kernel?","text":"<ul> <li>Controls the hardware when the OS talks to the kernel via system calls. The    kernel translates these requests into instructions that the hardware can    understand. </li> <li>Allocates memory and schedules processes to run applications.</li> <li>First piece of software loaded into a protected area of memory when a computer starts up so that it cannot be overwritten.</li> </ul> <p>In the command <code>ps -ef</code> PID 1 is the initial process started by the kernel. </p>","tags":["networking","linux"]},{"location":"insights/2023/18-05-2023/linux-primer/#what-is-a-system-call","title":"What is a system call?","text":"<p>When an application is run, by default it runs in user space. When it requires access to hardware like disk for example it must make a request to the kernel which is known as a \"system call\". </p> <p>Here are some common scenarios where applications make system calls to the kernel:</p> <p>File operations: When an application needs to read from or write to files, it makes system calls to the kernel to perform file-related operations, such as opening files, reading data, writing data, closing files, and modifying file attributes.</p> <p>Network communication: Applications that require network connectivity, such as web browsers or email clients, make system calls to the kernel to establish network connections, send data over the network, receive incoming data, and manage network sockets.</p> <p>Process management: Applications may need to create new processes, terminate processes, or perform other process-related operations. These tasks involve system calls to the kernel, which handles process scheduling, memory management, and inter-process communication.</p> <p>Memory management: When an application requires memory allocation or deallocation, it relies on system calls to the kernel to request memory resources. The kernel manages the system's memory and fulfills these requests, ensuring proper memory allocation and protection.</p> <p>Interacting with devices: Applications make system calls to interact with hardware devices like disks, printers, graphics cards, and input/output devices. These system calls enable the application to perform operations on the devices with the assistance of the kernel and relevant device drivers.</p> <p>If you run an application as root, it still runs within the user space but is granted elevated privilges that normally would not exist like modifying system files, changing system level configuration etc.</p>","tags":["networking","linux"]},{"location":"insights/2023/18-05-2023/linux-primer/#files","title":"Files","text":"<ul> <li><code>/bin</code>, <code>/sbin</code>, <code>/usr/bin</code>, and <code>/usr/sbin</code>: Where executable programs are stored.</li> <li><code>/dev</code>: Where files representing hardware devices are stored. For example, if your Linux system had a floppy drive device, there would be a file named fd0 in the dev folder (/dev/fd0).</li> <li><code>/etc</code>: Where configuration files are stored.</li> <li><code>/home</code>: Where user home directories are stored, one for each user.</li> <li><code>/var</code>. Where variable-length files, like log files, are stored.</li> </ul> <p>Should follow File System Hierachy guide as per https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard</p>","tags":["networking","linux"]},{"location":"insights/2023/18-05-2023/linux-primer/#log-files","title":"Log files","text":"<p><code>syslog</code>: Contains the centralized logging system, called syslog, in which you\u2019ll find messages related to the kernel, applications, and more. If configured, this could be the centralized log file for all Linux systems (or even all network devices) in your data center.</p> <p><code>auth.log</code>: Contains authentication failures and successes</p> <p><code>messages</code>: Contains general system messages of all types</p>","tags":["networking","linux"]},{"location":"insights/2023/18-05-2023/linux-primer/#network-interfaces","title":"Network interfaces","text":"<p>The loopback (<code>lo</code>) interface will have an IP address of 127.0.0.1, which represents the host itself.</p> <p>The ethernet 0 (<code>eth0</code>) interface is typically the connection to the local network. Even if you are running Linux in a virtual machine (VM), you\u2019ll still have an <code>eth0</code> interface that connects to the physical network interface of the host. Most commonly, you should ensure that eth0 is in an UP state and has an IP address so that you can communicate with the local network and likely over the Internet.</p>","tags":["networking","linux"]},{"location":"insights/2023/18-05-2023/linux-primer/#commands","title":"Commands","text":"<p><code>ip link</code>         : Configure network interfaces and check link status  <code>ip addr ...</code>     : Check and configure ip addresses for network interfaces <code>ip -s link</code>      : Stats on our network e.g: How much data is sent? any errors? etc. <code>netstat -l</code>      : Active listening services <code>ip neighbour</code>    : ARP cache table (IP to MAC) <code>ifup/ifdown</code> .   : Restart an interfaces without having to restart a servero</p>","tags":["networking","linux"]},{"location":"insights/2023/18-05-2023/linux-primer/#container-networking-made-simple","title":"Container networking made simple","text":"<p>The following are notes from the article https://iximiuz.com/en/posts/container-networking-is-simple/ (which is a must read).</p> <p><sub>Disclaimer: The notes are my own and if there are mistakes it is not relfective of the article.</sub></p>","tags":["networking","linux"]},{"location":"insights/2023/18-05-2023/linux-primer/#what-is-a-linux-network-stack","title":"What is a linux network stack?","text":"<p>An isolated network device, routing rules and any filters set by ip tables. The isolation provided by linux via a network namespace can be setup via the command <code>ip netns</code>.</p> <p>The man page for this command says:</p> <p>A network namespace is logically another copy of the network stack, with its own routes, firewall rules, and network devices.</p> <p>This is captured in the following script:</p> <p><pre><code>cat &lt;&lt;EOF &gt; inspect-net-stack.sh\n#!/usr/bin/env bash\necho \"&gt; Network devices\"\nip link\n\necho -e \"\\n&gt; Route table\"\nip route\n\necho -e \"\\n&gt; Iptables rules\"\niptables --list-rules\nEOF\n</code></pre> Set permissions:</p> <pre><code>chmod +x inspect-net-stack.sh\n</code></pre> <p>Create a custom ip tables rule to see the difference between the root network namespace and the custom namespace that will be created.</p> <pre><code>sudo iptables -N NS_ROOT\n</code></pre> <p>When you run this on the host (root network namespace) on a machine the output may look like this:</p> <pre><code>&gt; Network devices\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000\n    link/ether 52:54:00:27:8b:50 brd ff:ff:ff:ff:ff:ff\n\n&gt; Route table\ndefault via 10.0.2.2 dev eth0 proto dhcp metric 100\n10.0.2.0/24 dev eth0 proto kernel scope link src 10.0.2.15 metric 100\n\n&gt; Iptables rules\n-P INPUT ACCEPT\n-P FORWARD ACCEPT\n-P OUTPUT ACCEPT\n-N ROOT_NS\n</code></pre>","tags":["networking","linux"]},{"location":"insights/2023/18-05-2023/linux-primer/#how-to-create-a-network-namespace","title":"How to create a network namespace?","text":"<pre><code>$ sudo ip netns add netns0\n</code></pre>","tags":["networking","linux"]},{"location":"insights/2023/18-05-2023/linux-primer/#how-do-i-use-this-namespace","title":"How do I use this namespace?","text":"<pre><code># Run bash in the namespace we created\n$ sudo nsenter --net=/var/run/netns/netns0 bash\n\n$ sudo ./inspect-net-stack.sh\n&gt; Network devices\n1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n\n&gt; Route table\n\n&gt; Iptables rules\n-P INPUT ACCEPT\n-P FORWARD ACCEPT\n-P OUTPUT ACCEPT\n</code></pre> <p>As you can see there is only a loopback interface (which is also <code>DOWN</code>), no routing rules and the custom ip tables chain <code>NS_ROOT</code> is not present which confirms that we are in the <code>netns0</code> networking stack.</p> <p>At this point it is completely isolated and there is no network connectivity by default.</p>","tags":["networking","linux"]},{"location":"insights/2023/18-05-2023/linux-primer/#how-to-make-this-network-namespace-useful","title":"How to make this network namespace useful?","text":"<p>To enable connectivity to the <code>netns0</code> namepace, we need to create a form of link to the root namespace. Linux provides a way to make this happen via virtual ethernet devices.</p> <p>The man page for <code>veth</code> says:</p> <p>The  veth devices are virtual Ethernet devices.  They can act as tunnels between network namespaces to create a bridge to a physical network device in another namespace, but can also be used as standalone network devices.</p> <p>Create a <code>veth</code> on the root networking namespace:</p> <pre><code>$ sudo ip link add veth0 type veth peer name ceth0\n</code></pre> <p>The command highlights that we are creating the veth in pairs; veth0/ceth0. The purpose of creating veth interfaces in pairs is to establish a communication channel between different network namespaces. One end of the veth pair is typically assigned to the host's network namespace, while the other end is assigned to a specific network namespace, such as a container or another network namespace.</p> <p>Running the network commands from earlier now shows:</p> <pre><code>sudo ./inspect-net-stack.sh\n&gt; Network devices\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000\n    link/ether 52:54:00:27:8b:50 brd ff:ff:ff:ff:ff:ff\n3: ceth0@veth0: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000\n    link/ether da:c7:10:9f:f7:3c brd ff:ff:ff:ff:ff:ff\n4: veth0@ceth0: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000\n    link/ether 8a:09:3b:a1:50:5d brd ff:ff:ff:ff:ff:ff\n\n&gt; Route table\ndefault via 10.0.2.2 dev eth0 proto dhcp metric 100\n10.0.2.0/24 dev eth0 proto kernel scope link src 10.0.2.15 metric 100\n\n&gt; Iptables rules\n-P INPUT ACCEPT\n-P FORWARD ACCEPT\n-P OUTPUT ACCEPT\n-N ROOT_NS\n</code></pre> <p>To connect the root networking namespace to the <code>netns0</code> namespace we need to move one of the pairs over:</p> <pre><code>$ sudo ip link set ceth0 netns netns0\n\n# Confirm it is not longer in the root namespace\n$ ip link show\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000\n    link/ether 52:54:00:27:8b:50 brd ff:ff:ff:ff:ff:ff\n4: veth0@if3: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000\n    link/ether 8a:09:3b:a1:50:5d brd ff:ff:ff:ff:ff:ff link-netns netns0\n</code></pre>","tags":["networking","linux"]},{"location":"insights/2023/18-05-2023/linux-primer/#enabling-connectivity","title":"Enabling connectivity","text":"<p>From the root networking namespace we can see that the device is <code>DOWN</code> and there is no IP address assigned:</p> <pre><code>4: veth0@if3: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000\n    link/ether 8a:09:3b:a1:50:5d brd ff:ff:ff:ff:ff:ff link-netns netns0\n</code></pre> <p>We can fix that:</p> <pre><code>sudo ip link set veth0 up\nsudo ip addr add 172.18.0.11/16 dev veth0\n</code></pre> <p>IP Addressing</p> <p>Any ip address can be assigned to the interface because it is virtual. It doesn't have to be a real IP address that corresponds to a physical network device.</p> <p>LOWER_LAYER_DOWN</p> <p>If you had run <code>ip link show veth0</code>, you will notice that the link state is <code>LOWERLAYERDOWN</code>. This is expected   since the other end of the pair, <code>ceth0</code> is down.</p> <p>Doing the same for <code>ceth0</code> in <code>netns0</code> namespace:</p> <pre><code>sudo nsenter --net=/var/run/netns/netns0\nip link set lo up\nip link set ceth0 up\nip addr add 172.18.0.10/16 dev ceth0\n</code></pre> <p>Review connectivity:</p> <pre><code>$ ip link\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n3: ceth0@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000\n    link/ether 82:95:96:e2:c0:00 brd ff:ff:ff:ff:ff:ff link-netnsid 0\n</code></pre>","tags":["networking","linux"]},{"location":"insights/2023/18-05-2023/linux-primer/#testing-connectivity","title":"Testing connectivity","text":"<p>From <code>netns0</code> namespace to <code>veth0</code>:</p> <p><pre><code>sudo nsenter --net=/var/run/netns/netns0\n</code></pre> Ping <code>veth0</code> in host namespace:</p> <pre><code>$ ping -c 2 172.18.0.11\nPING 172.18.0.10 (172.18.0.10) 56(84) bytes of data.\n64 bytes from 172.18.0.10: icmp_seq=1 ttl=64 time=0.073 ms\n64 bytes from 172.18.0.10: icmp_seq=2 ttl=64 time=0.046 ms\n...\n</code></pre> <p>Make sure to exit from <code>ns0</code> namespace:</p> <pre><code>exit\n</code></pre> <p>From root namespace to <code>ceth0</code>:</p> <pre><code>$ ping -c 2 172.18.0.10\nPING 172.18.0.11 (172.18.0.11) 56(84) bytes of data.\n64 bytes from 172.18.0.11: icmp_seq=1 ttl=64 time=0.038 ms\n64 bytes from 172.18.0.11: icmp_seq=2 ttl=64 time=0.040 ms\n...\n</code></pre> <p>From <code>netns0</code> to root namespace <code>eth0</code> interface:</p> <pre><code># Get eth0 ip\n$ ip addr show dev eth0\n2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000\n    link/ether 52:54:00:e3:27:77 brd ff:ff:ff:ff:ff:ff\n    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic noprefixroute eth0\n       valid_lft 84057sec preferred_lft 84057sec\n    inet6 fe80::5054:ff:fee3:2777/64 scope link\n       valid_lft forever preferred_lft forever\n</code></pre> <p>Switch to <code>ns0</code> namespace:</p> <pre><code>sudo nsenter --net=/var/run/netns/netns0\n</code></pre> <pre><code># Try host's eth0\n$ ping 10.0.2.15\nconnect: Network is unreachable\n\n# Try something from the Internet\n$ ping 8.8.8.8\nconnect: Network is unreachable\n</code></pre> <p>To understand why you can't ping the host's <code>eth0</code> interface you, we need to verify if a route exists. </p> <pre><code>$ ip route\n172.18.0.0/16 dev ceth0 proto kernel scope link src 172.18.0.10\n</code></pre> <p>There is only a route to reach <code>172.18.0.0/16</code> network. To fix this we can add a default route:</p> <pre><code>ip route add default via 172.18.0.11\n</code></pre> <p>Default Route</p> <p>The default route is the next hop. It is not an interface on your host. Think about a router in your home network for example to reach the internet.</p> <p>Now if ping <code>eth0</code> it will have a way to reach it:</p> <pre><code>$ ping -c2 10.0.2.15\nPING 10.0.2.15 (10.0.2.15) 56(84) bytes of data.\n64 bytes from 10.0.2.15: icmp_seq=1 ttl=64 time=0.040 ms\n64 bytes from 10.0.2.15: icmp_seq=2 ttl=64 time=0.062 ms\n...\n</code></pre> <p>However, accessing the internet as per our <code>ping 8.8.8.8</code> command still fails.</p>","tags":["networking","linux"]},{"location":"insights/2023/18-05-2023/linux-primer/#nat","title":"NAT","text":"<p>The article linked above describes it best:</p> <p>Before going to the external network, packets originated by the containers will get their source IP addresses replaced with the host's external interface address. The host also will track all the existing mappings and on arrival, it'll be restoring the IP addresses before forwarding packets back to the containers.</p> <p>This is achieved by the following <code>iptables</code> rule:</p> <pre><code>sudo sysctl net.ipv4.ip_forward=1\nsudo iptables -t nat -A POSTROUTING -s 172.18.0.0/16 -o eth0 -j MASQUERADE\n</code></pre> <p>This rule ensures that any packet originating from the <code>172.18.0.0/16</code> network and going out through the <code>eth0</code> interface will have its source IP address changed to match the IP address of the <code>eth0</code> interface. This allows devices in the <code>172.18.0.0/16</code> network to communicate with the internet using the IP address of the <code>eth0</code> interface.</p> <p>To be continued..</p>","tags":["networking","linux"]},{"location":"insights/2023/18-05-2023/linux-primer/#summary","title":"Summary","text":"<pre><code>sudo ip netns add netns0\nsudo ip link add veth0 type veth peer name ceth0\nsudo ip link set veth0 up\nsudo ip addr add 172.18.0.11/16 dev veth0\nsudo ip link set ceth0 netns netns0\nsudo nsenter --net=/var/run/netns/netns0\nip link set lo up\nip link set ceth0 up\nip addr add 172.18.0.10/16 dev ceth0\nip route add default via 172.18.0.11\nexit\n\nsudo ip netns add netns1\nsudo ip link add veth1 type veth peer name ceth1\nsudo ip link set veth1 up\nsudo ip addr add 172.18.0.21/16 dev veth1\nsudo ip link set ceth1 netns netns1\n\nsudo nsenter --net=/var/run/netns/netns1\nip link set lo up\nip link set ceth1 up\nip addr add 172.18.0.20/16 dev ceth1\nip route add default via 172.18.0.21\n# my host eth0 ip is 10.0.2.15\narping -c 1 -I ceth1 10.0.2.15\nexit\n\nsudo ip route del 172.18.0.0/16 dev veth0 proto kernel scope link src 172.18.0.11\nsudo nsenter --net=/var/run/netns/netns1\narping -c 1 -I ceth1 10.0.2.15\n</code></pre>","tags":["networking","linux"]},{"location":"insights/2023/19-04-2023/context-timeout/","title":"Go Context Timeout","text":"","tags":["go","client","context"]},{"location":"insights/2023/19-04-2023/context-timeout/#when-does-the-contexcontext-cancel-for-withtimeout","title":"When does the <code>contex.Context</code> cancel for <code>WithTimeout()</code> ?","text":"<p>If you have <code>funcA()</code> calling <code>funcB()</code> with a context timeout, the timer starts from the time the context is created and you will get a <code>DeadlineExceeded</code> error if the timer expires.</p> <p>typically you add a <code>defer cancel()</code> to ensure there is no context leak. If you are calling <code>funcB()</code> via a goroutine you must wrap the <code>defer cancel()</code> inside the goroutine. Otherwise, you will get <code>DeadlineExceeded</code> error.</p> <p>Example</p> <pre><code>package main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"time\"\n)\n\nfunc funcA() {\n    ctx, cancel := context.WithTimeout(context.Background(), time.Second*3)\n    go func() {\n        defer cancel() // Defer canceling the context\n        funcB(ctx)\n    }()\n}\n\nfunc funcB(ctx context.Context) {\n    select {\n    case &lt;-time.After(2 * time.Second):\n        // Do something that takes more than 2 seconds\n        fmt.Println(\"Work for 2 seconds\")\n    case &lt;-ctx.Done():\n        // Context canceled or timed out\n        if ctx.Err() == context.DeadlineExceeded {\n            fmt.Println(context.DeadlineExceeded)\n        } else {\n            fmt.Println(ctx.Err())\n        }\n    }\n\n}\nfunc main() {\n    funcA()\n    time.Sleep(time.Second * 3)\n\n}\n</code></pre>","tags":["go","client","context"]},{"location":"insights/2023/19-04-2023/reusing-http-client/","title":"Go Reusing HTTP Client","text":"","tags":["api-design","go","client"]},{"location":"insights/2023/19-04-2023/reusing-http-client/#clear-apis-and-reusing-the-http-client","title":"Clear APIs and reusing the http client","text":"<p>When designing an API it is critical to consider the usability of the API from the user's or developer's perspective. With this requirement in mind, when designing the usage of a HTTP client for a service, which may have  multiple sub-services, it is useful to provid a scoped API yet reuse the same HTTP client.</p> <p>I was looking for a pattern to do this and found the following library solving this in an interesting way:</p> <p>https://github.com/raksul/go-clickup/blob/main/clickup/client.go</p> <p>Let's look at the interesting bits.</p> <p>Declartion in <code>NewClient()</code> with my comments:</p> <pre><code>// Define a client. Nothing new here.\nc := &amp;Client{client: httpClient, BaseURL: baseURL, UserAgent: userAgent, APIKey: APIKey}\n// Now assign the client to common.\nc.common.client = c\n// We don't want to allocate to the heap so we assign the address of c.common to c.Attachments after casting\n// it as a pointer to AttachmentsService.\n// This means that c.Attachments now references the same object in memory as c.common.\nc.Attachments = (*AttachmentsService)(&amp;c.common)\n// Do the same for all the sub-services.\nc.Authorization = (*AuthorizationService)(&amp;c.common)\nc.Checklists = (*ChecklistsService)(&amp;c.common)\nc.Comments = (*CommentsService)(&amp;c.common)\n...\n</code></pre> <p>Corresponding defnitions for above code:</p> <pre><code>type service struct {\n        client *Client\n}\n\n\ntype AttachmentsService service\n\n\ntype Client struct {\n        clientMu sync.Mutex   // clientMu protects the client during calls that modify the CheckRedirect func.\n        client   *http.Client // HTTP client used to communicate with the API.\n        APIKey   string\n\n        BaseURL   *url.URL\n        UserAgent string\n\n        rateMu     sync.Mutex\n        rateLimits Rate // Rate limits for the client as determined by the most recent API calls.\n\n        common service // Reuse a single struct instead of allocating one for each service on the heap.\n\n        // Services used for talking to different parts of the Clickup API.\n        Attachments     *AttachmentsService\n        Authorization   *AuthorizationService\n        Checklists      *ChecklistsService\n        Comments        *CommentsService\n        ...\n}\n</code></pre> <p>This can be illustrated as shown below (for <code>Attachments</code>):</p> <p></p> <p>The takeaway being you can typecast and assign the memory address of <code>common</code> to <code>Attachments</code> because they have the same <code>Client</code> type.</p> <p>The API is now accessible as shown in the following pseudocode:</p> <pre><code>c := NewClient()\nc.Attachments.* // Attachments specific APIs \nc.Comments.* // Comments specific APIs\n</code></pre>","tags":["api-design","go","client"]},{"location":"insights/2023/20-09-2023/istio-egress-tls-origination/","title":"Istio Egress TLS Origination","text":"","tags":["istio","tls"]},{"location":"insights/2023/20-09-2023/istio-egress-tls-origination/#overview","title":"Overview","text":"<p>Illustrate how to configure Istio to handle egress traffic to external services for path based routing setup via a Gateway.</p> <pre><code>Client (HTTPS) LB - SSL offload   --&gt; (HTTP) Istio-ingressgateway  --&gt; (HTTPS) External-service\n                                                                   --&gt; (HTTP)  Internal-service\n</code></pre> <p>When the incoming request to Istio is HTTP (rather than HTTPS), Istio does not automatically upgrade the request to HTTPS when connecting to the external service. This is also briefly mentioned in the documentation here:</p> <p>TLS origination occurs when an Istio proxy (sidecar or egress gateway) is configured to accept unencrypted internal HTTP connections, encrypt the requests, and then forward them to HTTPS servers that are secured using simple or mutual TLS.</p>","tags":["istio","tls"]},{"location":"insights/2023/20-09-2023/istio-egress-tls-origination/#serviceentry","title":"ServiceEntry","text":"<p>Setup a ServiceEntry to enable access to n S3 bucket. The S3 bucket in this instance is our external service. This is a common configuration for accessing external services from Istio.</p> <p>Info</p> <p>The S3 bucket in this example has been configured to serve requests on paths <code>/.well-known/openid-configuration</code> and <code>/oauth/discovery/keys</code>.</p> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: ServiceEntry\nmetadata:\n  name: s3\n  namespace: istio-system\nspec:\n  hosts:\n    - samplebucket1234.s3.ap-southeast-2.amazonaws.com\n  location: MESH_EXTERNAL\n  exportTo:\n  - \".\"\n  ports:\n    - number: 443\n      name: https\n      protocol: HTTPS\n  resolution: DNS\n</code></pre>","tags":["istio","tls"]},{"location":"insights/2023/20-09-2023/istio-egress-tls-origination/#gateway","title":"Gateway","text":"<p>Setup a Gateway that accepts HTTP traffic. It is SSL offloaded at the LB. </p> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: example-gateway\nspec:\n  selector:\n    istio: ingressgateway  # use Istio default gateway implementation\n  servers:\n    - port:\n        number: 80\n        name: http\n        protocol: HTTP\n      hosts:\n        - example.com\n</code></pre>","tags":["istio","tls"]},{"location":"insights/2023/20-09-2023/istio-egress-tls-origination/#virtualservice","title":"VirtualService","text":"<p>In this example, the root path <code>/</code> requests should route to the <code>welcome</code> internal service  and <code>/.well-known/openid-configuration</code> and <code>/oauth/discovery/keys</code> to the external service. To provide this traffic routing a VirtualService is configured as shown below.</p> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: s3\nspec:\n  gateways:\n  - example-gateway\n  http:\n    - match:\n        - uri:\n            prefix: /\n      route:\n        - destination:\n            host: welcome  # Name of the internal service\n            port:\n              number: 8080\n    - match:\n        - uri:\n            prefix: /.well-known/openid-configuration \n      route:\n        - destination:\n            host: samplebucket1234.s3.ap-southeast-2.amazonaws.com\n            port:\n              number: 443\n      headers:\n        request:\n          set:\n            X-Forwarded-Proto: https\n            Host: samplebucket1234.s3.ap-southeast-2.amazonaws.com          \n    - match:\n        - uri:\n            prefix: /oauth/discovery/keys\n      route:\n        - destination:\n            host: samplebucket1234.s3.ap-southeast-2.amazonaws.com\n            port:\n              number: 443\n      headers:\n        request:\n          set:\n            X-Forwarded-Proto: https\n            Host: samplebucket1234.s3.ap-southeast-2.amazonaws.com          \n</code></pre> <p>At this point if you try to access the external services i.e: <code>example.com/.well-known/openid-configuration</code> or <code>example.com/oauth/discovery/keys</code> you will get the following error:</p> <pre><code>upstream connect error or disconnect/reset before headers. reset reason: protocol error\n</code></pre> <p>This is because the external service is expecting the request to be HTTPS. Istio does not perform TLS origination automatically. This has to be configured explicitly via a DestinationRule.</p>","tags":["istio","tls"]},{"location":"insights/2023/20-09-2023/istio-egress-tls-origination/#destinationrule","title":"DestinationRule","text":"<p>Setup a DestinationRule to perform TLS origination for HTTPS requests on port 443.</p> <pre><code>---\napiVersion: networking.istio.io/v1alpha3\nkind: DestinationRule\nmetadata:\n  name: s3\n  namespace: istio-system\nspec:\n  host: samplebucket1234.s3.ap-southeast-2.amazonaws.com\n  exportTo:\n  - \".\"\n  trafficPolicy:\n    portLevelSettings:\n    - port:\n        number: 443\n      tls:\n        mode: SIMPLE # initiates HTTPS when accessing samplebucket1234.s3.ap-southeast-2.amazonaws.com\n</code></pre> <p>The <code>tls.mode</code> setting when set to <code>SIMPLE</code> will initiate a HTTPS connection to the external service.</p>","tags":["istio","tls"]},{"location":"insights/2023/20-09-2023/istio-egress-tls-origination/#references","title":"References","text":"<p>-https://istio.io/latest/docs/tasks/traffic-management/egress/egress-tls-origination/</p>","tags":["istio","tls"]},{"location":"insights/2023/22-09-2023/ssl-certs-faq/","title":"SSL Certs","text":"","tags":["ssl","faq"]},{"location":"insights/2023/22-09-2023/ssl-certs-faq/#primer","title":"Primer","text":"<p>SSL certificates provide 2 distinct functions:</p> <ul> <li>Authenticity - \"Who am I connecting to?\"</li> <li>Privacy - \"Is my data private?\" i.e. encrypted.</li> </ul>","tags":["ssl","faq"]},{"location":"insights/2023/22-09-2023/ssl-certs-faq/#what-is-pki","title":"What is PKI?","text":"<p>Public Key Infrastructure (PKI) is a framework that enables secure communication and authentication over networks using a combination of public and private cryptographic keys. It involves the use of digital certificates, certificate authorities (CAs), and a set of policies and procedures to manage the creation, distribution, and revocation of these certificates.</p> <p>To break it down further it has the following components:</p> <p>1.Certificate Authority (CA): A trusted entity that issues digital certificates. CAs verify the identity of entities (individuals, organizations, or devices) before issuing certificates. 2. Registration Authority (RA): An entity that acts as an intermediary between the user and the CA. The RA is responsible for accepting requests for digital certificates and authenticating the entity making the request. 3. Public/Private Key Pair: Each entity has a pair of cryptographic keys - a public key and a private key. The public key is shared with others, while the private key is kept secret. Data encrypted with the public key can only be decrypted with the corresponding private key, and vice versa. 4. Digital Certificates: These are electronic documents that bind a public key to an entity's identity   (e.g., a person or organization). Certificates are issued by CAs and contain information such as the entity's name, public key, expiration date, and the CA's digital signature. 5. Certificate Store - A repository where trusted CA certificates are stored. This store is used by clients to verify the authenticity of digital certificates presented by servers during secure communications.</p>","tags":["ssl","faq"]},{"location":"insights/2023/22-09-2023/ssl-certs-faq/#faq","title":"FAQ","text":"","tags":["ssl","faq"]},{"location":"insights/2023/22-09-2023/ssl-certs-faq/#how-is-a-certificate-validated","title":"How is a certificate validated?","text":"<ol> <li>The domain name in the URL must match the domain name in the certificate.</li> <li>The certificate must be within its validity period (not expired).</li> <li>The certificate must be signed by a trusted Certificate Authority (CA).</li> </ol>","tags":["ssl","faq"]},{"location":"insights/2023/22-09-2023/ssl-certs-faq/#does-the-server-send-the-intermediate-ca-certs-in-the-ssl-handshake","title":"Does the server send the intermediate CA certs in the SSL handshake?","text":"<p>Short answer: Yes.</p> <p>See this StackOverflow answer.</p> <p>As per the RFC:</p> <pre><code>certificate_list\n  This is a sequence (chain) of certificates.  The sender's\n  certificate MUST come first in the list.  Each following\n  certificate MUST directly certify the one preceding it.  Because\n  certificate validation requires that root keys be distributed\n  independently, the self-signed certificate that specifies the root\n  certificate authority MAY be omitted from the chain, under the\n  assumption that the remote end must already possess it in order to\n  validate it in any case.\n</code></pre>","tags":["ssl","faq"]},{"location":"insights/2023/22-09-2023/ssl-certs-faq/#is-the-root-ca-cert-also-sent-by-the-server","title":"Is the Root CA cert also sent by the server?","text":"<p>It may not. The RFC says:</p> <p>The self-signed certificate that specifies the root certificate authority MAY be omitted from the chain, under the assumption that the remote end must already possess it in order to validate it in any case.</p> <p>So the client must have the Root CA cert installed in its trust store. Typically, all major Root CA  certificates are bundled into the client.</p>","tags":["ssl","faq"]},{"location":"insights/2023/22-09-2023/ssl-certs-faq/#how-do-i-view-the-certs-sent-by-the-server","title":"How do I view the certs sent by the server?","text":"<pre><code>openssl s_client -connect google.com:443 -showcerts &lt; /dev/null 2&gt;/dev/null\n</code></pre>","tags":["ssl","faq"]},{"location":"insights/2023/22-09-2023/ssl-certs-faq/#how-does-the-server-send-the-ca-certs","title":"How does the server send the CA certs?","text":"<p>It is bundled into what's known as the fulll certificate chain.  As shown in the example below, the server certificate must come first, followed by any intermediary CA certs.</p> <pre><code>cat server_cert.pem intermediate1.pem intermediate2.pem &gt; fullchain.pem\n</code></pre>","tags":["ssl","faq"]},{"location":"insights/2023/22-09-2023/ssl-certs-faq/#what-is-the-default-path-to-the-intermediate-ca-certs","title":"What is the default path to the intermediate CA certs?","text":"<p>The default path is <code>/etc/ssl/certs/ca-certificates.crt</code> on Debian and Ubuntu.</p>","tags":["ssl","faq"]},{"location":"insights/2023/22-09-2023/ssl-certs-faq/#what-is-the-chain-of-trust","title":"What is the Chain of Trust?","text":"<p>In the SSL handshake, when the browser receives the server's certificate, it needs to check if the certificate is signed by a trusted CA. The process of how this happens is outlined below.</p> <ol> <li>During the handshake, the client looks at the leaf certificate, which is typically signed by an intermediate CA.</li> <li>The browser will look for the public certificate of the intermediate CA to verify the signature of the leaf certificate. If it already exists in the trust store, it can immediately complete signature verification by using the CA's public key. </li> <li>If the intermediate CA's public key is not found in the trust store, the browser must get the intermediate CA certificate via the certificate list in the leaf cert by looking at the issuer. </li> <li>The browser would now have to verify the signature for the intermediate CA (via the parent CA that signed the intermediate CA cert) to complete the chain of trust. The browser can identify the issuer of the intermediate CA cert by searching in its trust store for the public cert of the issuer. If it finds it, it will use it to verify the signature of the intermediate CA cert which completes the chanin of trust.</li> <li>If it is not found in the trust store, step 3-4 will be repeated. This process stops when the browser either has the public key of the top-level intermediate CA that signed the intermediate CA to the leaf certificate or the issuer is the Root CA and verifies the signature because it has the Root CA public key in its trust store.</li> </ol> <p>If any step fails in this process (e.g., a certificate is expired, revoked, or the signature is invalid), the client will not trust the certificate, and the connection may be terminated or a warning message displayed.</p>","tags":["ssl","faq"]},{"location":"insights/2023/22-09-2023/ssl-certs-faq/#how-do-you-retrieve-the-certificates-from-a-domain-programatically","title":"How do you retrieve the certificates from a domain programatically?","text":"<pre><code>openssl s_client -showcerts -connect  foo.com -servername  foo.com  &lt;/dev/null | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' &gt; foo.com.pem\n</code></pre>","tags":["ssl","faq"]},{"location":"insights/2023/22-09-2023/ssl-certs-faq/#what-is-the-difference-between-signing-a-certificate-and-encrypting","title":"What is the difference between signing a certificate and encrypting?","text":"<p>They are related in that they use the same encryption algorithm e.g: RS256. </p> <p>However they serve different purposes.</p> <p>Signing: Used to maintain integrity and authenticity when the CA signs a certificate or an OIDC server signs a token.  The data is hashed and then signed by the server's private key. The client or relying party uses the public key to expose the hashed data by decrypting the signature or message. When we refer to \"signing\", we generally mean to use a private key to sign a piece of data that can be extracted via the public key.</p> <p>Encryption: Use to maintain confidentiality where the client encrypts the data using the server's public key (available to the client) that is then decrypted by the server's private key. </p>","tags":["ssl","faq"]},{"location":"insights/2023/22-09-2023/ssl-certs-faq/#what-is-a-self-signed-certificate","title":"What is a Self Signed Certificate?","text":"<p>A self-signed certificate is a digital certificate that is signed by the same entity whose identity it certifies. In other words, the issuer and the subject of the certificate are the same. Self-signed certificates are typically used for testing, development, or internal purposes where trust can be established without relying on a third-party Certificate Authority (CA).</p> <p>It is worth noting that self-signed certificates do not provide the same level of trust as certificates issued by a trusted CA, as they are not validated by an external authority. As a result, web browsers and other clients may display warnings when encountering self-signed certificates, indicating that the connection may not be secure.</p>","tags":["ssl","faq"]},{"location":"insights/2023/23-12-2023/Terraform%20Primer/","title":"Q&amp;A","text":"","tags":["terraform"]},{"location":"insights/2023/23-12-2023/Terraform%20Primer/#what-are-3-ways-you-can-use-a-data-source","title":"What are 3 ways you can use a data source?","text":"<p>There are a few different ways data sources can be used:</p> <p>Referencing a resource managed by Terraform:</p> <pre><code>resource \"aws_s3_bucket\" \"example\" {\n  # ...\n}\n\ndata \"aws_s3_bucket\" \"example\" {\n  bucket = aws_s3_bucket.example.id \n}\n</code></pre> <p>Here the data source is looking up attributes of the S3 bucket managed in this Terraform configuration.</p> <pre><code>Referencing resources not managed by Terraform:\ndata \"aws_s3_bucket\" \"existing\" {\n  bucket = \"my-existing-bucket\" \n}\n</code></pre> <p>Here the data source is fetching data about an S3 bucket that already exists outside of Terraform.</p> <p>Lookup based on filters:</p> <pre><code>data \"aws_ami\" \"app\" {\n  filter {\n    name = \"name\"\n    values = [\"app-ami-*\"]\n  }\n}\n</code></pre> <p>Here the data source will find the latest AMI that matches the given filter criteria.</p> <p>So in summary - data sources can reference Terraform resources, external resources not managed by Terraform, or lookup resources based on criteria like tags or filters. They don't strictly require a Terraform resource reference.</p>","tags":["terraform"]},{"location":"insights/2023/23-12-2023/Terraform%20Primer/#how-do-you-delete-a-single-resource-in-a-module","title":"How do you delete a single resource in a module?","text":"<pre><code>terraform state list\n</code></pre> <p>dynamic zone uses a protocol to do the updates. uses an auth key</p> <p>DNS hierachy hostname.appsec. hostname.prod.appspec.</p> <p>zone cut.</p>","tags":["terraform"]},{"location":"insights/2024/13-10-2024/tls-troublehsooting/","title":"TLS Troubleshooting","text":"","tags":["tls","ssl"]},{"location":"insights/2024/13-10-2024/tls-troublehsooting/#basics","title":"Basics","text":"","tags":["tls","ssl"]},{"location":"insights/2024/13-10-2024/tls-troublehsooting/#what-is-a-tls-session","title":"What is a TLS session?","text":"<p>A session is created when a TLS handshake is established. This allows the client and server to have a unique set of encryption keys that are derived during the handhshake, known as session keys. Even if somoneone were to intercept the the traffic the session keys will not be able to decrypt other sessions. This is known Perfect Forward Secrecy (PFS) because each session is independent. </p> <p>A session is established when:   - Initial connection to website    - Session resumption - Returning to the website may resume an existing session </p> <p>Scenarios that can trigger a new session:</p> <ul> <li>New browser tab</li> <li>Determined by server configuration </li> <li>Subdomains</li> </ul>","tags":["tls","ssl"]},{"location":"insights/2024/13-10-2024/tls-troublehsooting/#establishing-a-tls-session","title":"Establishing a TLS session","text":"<pre><code>@startuml\ntitle TLS Handshake with Algorithms\n\nactor Client\nactor Server\nparticipant \"CA\" as CertificateAuthority\n\n== Step 1: Client Hello ==\nClient -&gt; Server: ClientHello (Cipher suites, Key Exchange Algorithm)\nnote right of Client\nClient proposes algorithms for:\n- Key Exchange Algorithm (e.g., ECDHE, RSA)\n- Encryption Algorithm (e.g., AES)\n- Hashing Algorithm (e.g., SHA-256)\nend note\n\n== Step 2: Server Hello ==\nServer -&gt; Client: ServerHello (Chosen Cipher Suite, Key Exchange Algorithm)\nnote right of Server\nServer selects the cipher suite including:\n- Key Exchange Algorithm (e.g., ECDHE)\n- Encryption Algorithm (e.g., AES)\n- Hashing Algorithm (e.g., SHA-256)\nend note\n\n== Step 3: Server Certificate ==\nServer -&gt; Client: Certificate (Signed by CA)\nnote right of Client\nClient verifies the certificate using the CA's public key:\n- Authentication Algorithm (e.g., RSA, ECDSA)\nend note\n\n== Step 4: Server Key Exchange ==\nServer -&gt; Client: ServerKeyExchange (Ephemeral Public Key)\nnote right of Server\nEphemeral key is used for:\n- Key Exchange Algorithm (e.g., ECDHE)\nend note\n\n== Step 5: Client Key Exchange ==\nClient -&gt; Server: ClientKeyExchange (Pre-master secret encrypted)\nnote right of Client\nPre-master secret is encrypted using:\n- Authentication Algorithm (e.g., RSA, ECDSA)\nend note\n\n== Step 6: Generate Session Keys ==\nClient -&gt; Client: Generate session keys using key material and Hashing Algorithm (e.g., SHA-256)\nServer -&gt; Server: Generate session keys using key material and Hashing Algorithm (e.g., SHA-256)\n\n== Step 7: Finished Messages ==\nClient -&gt; Server: Finished message (MAC using Hashing Algorithm)\nServer -&gt; Client: Finished message (MAC using Hashing Algorithm)\n\n== Step 8: Secure Communication ==\nClient &lt;--&gt; Server: Encrypted data transfer using Encryption Algorithm (e.g., AES)\n@enduml\n</code></pre>","tags":["tls","ssl"]},{"location":"insights/2024/13-10-2024/tls-troublehsooting/#what-is-ocsp-online-certificate-status-protocol-and-why-should-i-care","title":"What is OCSP (Online Certificate Status Protocol) and why should I care?","text":"<ul> <li> <p>Think of it as an API for checking revocation.</p> </li> <li> <p>The client sends the cert\u2019s serial number to an OCSP responder (usually the CA), asking:</p> </li> </ul> <p>\u201cIs this certificate still valid?\u201d</p> <ul> <li>The responder sends back a signed response saying: <code>good</code>, <code>revoked</code>, or <code>unknown</code>.</li> </ul> <p>OCSP responses include:</p> <p><code>thisUpdate</code>: when the status was generated.</p> <p><code>nextUpdate</code>: when the response expires.</p> <p>This is to ensure the client is not dealing with a stale response.</p>","tags":["tls","ssl"]},{"location":"insights/2024/13-10-2024/tls-troublehsooting/#time-sync-issue","title":"Time Sync Issue","text":"<p>A certificate handshake can fail due to time synchronoisation issues that is likely to be caused by OSCP check. In this case it is confirmed that the certificate expiry and system time is well within the <code>NotBefore</code> and <code>NotAfter</code> time. </p> <pre><code># How you would check this\nopenssl s_client -connect your-peer:443 &lt;/dev/null 2&gt;/dev/null | openssl x509 -noout -startdate -enddate\n</code></pre> <p>An example error you might see in your client logs:</p> <pre><code>failed to verify certificate: x509: certificate has expired or is not yet valid: current time 2025-06-04T23:43:47Z is before 2025-06-04T23:43:59Z\\\"\n</code></pre> <pre>3ee85a043c12c996d0a8e51e70e0f17b5deca21939e1536ff32c1ffa45541b35d8a6694a60593f39a1bec715dcb36ab5dc0542f641499571bde5236b860fd830</pre><pre>dd7457d41c71545396e146fce7a4472944bf4205abf87553ace2e8252db34194ab12abbcc4c3a99d5d8b9667f005608c816a1de113f052072091a1278c0880b5</pre>","tags":["tls","ssl"]},{"location":"insights/2024/13-10-2024/tls-troublehsooting/#commands","title":"Commands","text":"","tags":["tls","ssl"]},{"location":"insights/2024/13-10-2024/tls-troublehsooting/#supported-ciphers-by-server","title":"Supported ciphers by server","text":"<pre><code> nmap --script ssl-enum-ciphers -p 443 $HOST\n</code></pre>","tags":["tls","ssl"]},{"location":"insights/2024/13-10-2024/tls-troublehsooting/#specify-cipher","title":"Specify cipher","text":"<p><sub> Note: Ciphers can be discovered via <code>nmap</code> command above. </sub></p> <pre><code># TLS v1.2 \nopenssl s_client -connect $HOST:443 -no_tls1_3 -cipher TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\n\n# TLS v1.3\nopenssl s_client -connect $HOST:443 -tls1_3 -ciphersuites TLS_AES_256_GCM_SHA384\n</code></pre>","tags":["tls","ssl"]},{"location":"insights/2024/13-10-2024/tls-troublehsooting/#references","title":"References","text":"<ul> <li>https://www.cloudflare.com/learning/ssl/what-happens-in-a-tls-handshake/</li> <li>https://www.youtube.com/watch?v=C4Gtq5anlyc&amp;ab_channel=ChrisGreer</li> </ul>","tags":["tls","ssl"]},{"location":"insights/2024/26-04-2024/hashicorp-vault-keys-autounseal/","title":"Hashicorp Vault Keys for Autounseal","text":"<p>The first thing to remember is that when Vault starts, it is in a Sealed state. </p>","tags":["hashicorp","vault"]},{"location":"insights/2024/26-04-2024/hashicorp-vault-keys-autounseal/#what-does-sealed-state-mean","title":"What does Sealed state mean?","text":"<p>By configuration Vault knowns where and how to access the physical storage of secrets but cannot decrypt them. So it must be unsealed. This would mean it is encrypted in someway. Spot on, it is formally referred to as the Root key.</p> <p>This leads us back to unsealing, which is the process of getting access to the plaintext Root key. And why to do we need the Root key? because all data is encrypted with an Encryption key that is derived from the Root key.</p> <p>Unsealing is the process of getting access to the plaintext Root key.</p>","tags":["hashicorp","vault"]},{"location":"insights/2024/26-04-2024/hashicorp-vault-keys-autounseal/#ok-so-where-does-the-root-key-come-from","title":"Ok, So where does the Root key come from?","text":"<p>The root key is stored alongside other vault data. Which means from a security standpoint, it needs to be encrypted at rest. </p>","tags":["hashicorp","vault"]},{"location":"insights/2024/26-04-2024/hashicorp-vault-keys-autounseal/#so-theres-another-key-to-encrypt-the-root-key","title":"So there's another key to encrypt the Root key?","text":"<p>Yes, this is where the Unseal Key is relevant. The auto unseal feature delegates responsibility of securing the Unseal key to a trusted device or system. At startup Vault will connect to the device or system and ask it to decrypt the Root key it read from storage.</p>","tags":["hashicorp","vault"]},{"location":"insights/2024/26-04-2024/hashicorp-vault-keys-autounseal/#what-is-the-root-token-then","title":"What is the Root token then?","text":"<p>Root tokens are tokens that have the root policy attached to them. Root tokens can do anything in Vault. Anything.</p> <p>There are 3 ways root token can exist:</p> <ol> <li>When vault is initialised via <code>vault operation init</code></li> <li>Using an existing root token to create another </li> <li>By running <code>vault operator generate-root</code> using the Recovery keys</li> </ol>","tags":["hashicorp","vault"]},{"location":"insights/2024/26-04-2024/hashicorp-vault-keys-autounseal/#what-are-recovery-keys","title":"What are Recovery keys?","text":"<p>There are certain operations in Vault that require explicit authorization for it to be performed. For example, unsealing Vault or generating a Root token. When using Autounseal, this requires Recovery keys. </p>","tags":["hashicorp","vault"]},{"location":"insights/2024/26-04-2024/hashicorp-vault-keys-autounseal/#how-do-we-get-the-recovery-keys","title":"How do we get the Recovery keys?","text":"<p>When Vault is first initialised via Autounseal, it yield Recovery keys from the Root key. In other words, the Root key is split into a series of key shares following Shamir's Secret Sharing Algorithm.</p> <p>One of the first operational activities is to initialise Vault by running <code>vault operator init</code>. This will generate the Root key, the Recovery keys (as desribed) and the Root token.</p> <p>The process of generating a new Root key is called Rekeying.</p>","tags":["hashicorp","vault"]},{"location":"insights/2024/26-04-2024/hashicorp-vault-keys-autounseal/#explain-rekeying","title":"Explain Rekeying?","text":"<p>The process for generating a new root key and applying Shamir's algorithm is called \"rekeying\"</p> <pre><code>$vault operator rekey -target=recovery -init -key-shares=5 -key-threshold=3\n\nWARNING! If you lose the keys after they are returned, there is no recovery.\nConsider canceling this operation and re-initializing with the -pgp-keys flag\nto protect the returned unseal keys along with -backup to allow recovery of\nthe encrypted keys in case of emergency. You can delete the stored keys later\nusing the -delete flag.\n\nKey                      Value\n---                      -----\nNonce                    e0bd8648-0f8a-8641-bc18-2b4158d406a9\nStarted                  true\nRekey Progress           0/3\nNew Shares               5\nNew Threshold            3\nVerification Required    false\n</code></pre> <p>Provide the Recovery keys to meet the key threshold. One example is shown below:</p> <pre><code>$ vault operator rekey -target=recovery\n\nRekey operation nonce: e0bd8648-0f8a-8641-bc18-2b4158d406a9\nUnseal Key (will be hidden):\nKey                      Value\n---                      -----\nNonce                    e0bd8648-0f8a-8641-bc18-2b4158d406a9\nStarted                  true\nRekey Progress           1/3\nNew Shares               5\nNew Threshold            3\nVerification Required    false\n</code></pre> <p>Once all 3 Recovery keys have been applied, it will generate new Recovery keys from the new Root key.</p>","tags":["hashicorp","vault"]},{"location":"insights/2024/26-04-2024/hashicorp-vault-keys-autounseal/#how-do-we-generate-a-new-root-token","title":"How do we generate a new Root token?","text":"<p>A new Root token can be created with the Recovery keys.</p> <pre><code>$ vault operator generate-root -init\nA One-Time-Password has been generated for you and is shown in the OTP field.\nYou will need this value to decode the resulting root token, so keep it safe.\nNonce         c6f98535-43de-cd7b-d4d4-fd8fb17fd381\nStarted       true\nProgress      0/3\nComplete      false\nOTP           BJ4MR81PaVRw7fjLZqBti5H7dkiS\nOTP Length    28\n</code></pre> <p>Provide the Recovery keys to meet the key threshold. One example is shown below:</p> <pre><code>vault operator generate-root\nOperation nonce: c6f98535-43de-cd7b-d4d4-fd8fb17fd381\nUnseal Key (will be hidden):\nNonce       c6f98535-43de-cd7b-d4d4-fd8fb17fd381\nStarted     true\nProgress    1/3\nComplete    false\n</code></pre> <p>Once all 3 Recovery keys have been supplied, it will provided the encoded Root token as shown below:</p> <pre><code>vault operator generate-root\nOperation nonce: c6f98535-43de-cd7b-d4d4-fd8fb17fd381\nUnseal Key (will be hidden):\nNonce            c6f98535-43de-cd7b-d4d4-fd8fb17fd381\nStarted          true\nProgress         3/3\nComplete         true\nEncoded Token    KjxHYxxhXB1ZDwQuQVcgOi8HOhUfWS5BDgYbGA\n</code></pre> <p>The OTP from step 1 can be used to decode the Root token.</p> <pre><code>vault operator generate-root \\\n&gt; -decode=KjxHYxxhXB1ZDwQuQVcgOi8HOhUfWS5BDgYbGA \\\n&gt; -otp=BJ4MR81PaVRw7fjLZqBti5H7dkiS\n</code></pre> <p>To recap: most Vault data is encrypted using the encryption key in the keyring; the keyring is encrypted by the root key; and the root key is encrypted by the unseal key.</p> <pre><code>vault operator generate-root \\\n -decode=OgUXeGpAYHYIPRFaAQcsDgYQDQIdJRM9V3M8KQ \\\n -otp=RsdV3r8BCIIlMnXbCiZLJtbyg2Nj\n</code></pre>","tags":["hashicorp","vault"]},{"location":"insights/2025/18-08-2025/resiliency-framework/","title":"Resiliency Framework","text":""},{"location":"insights/2025/19-05-2025/fluxcd/","title":"Cilium","text":"","tags":["k8s","cillium","network-policy"]},{"location":"insights/2025/19-05-2025/fluxcd/#cilium-network-policy","title":"Cilium Network Policy","text":"<p>Oh man trying to deal with Cillium network policies can be painful when the cluster automatically enforces usage of it and you are trying to figure out  how to make it work. </p> <p>So turns out the cluster has  a <code>allow-dns</code> network policy but I was facing dns lookup errors.</p> <p><pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-dns\n  namespace: aws-load-balancer-controller  \nspec:\n  egress:\n  - ports:\n    - port: 53\n      protocol: UDP\n    - port: 53\n      protocol: TCP\n    to:\n    - namespaceSelector:\n        matchLabels:\n          name: kube-system\n  podSelector: {}\n  policyTypes:\n  - Egress\n  ```\n\nThis looks like it should work except that the `kube-system` namespace does not have that label.\n</code></pre> k get ns kube-system -o yaml | yq .metadata.labels kubernetes.io/metadata.name: kube-system <pre><code>To prove this further:\n</code></pre> k run -it dns-test-pod --image=busybox:1.36.1 --restart=Never --rm --namespace=vault-test -- sh -c \"nslookup kubernetes.default.svc.cluster.local\" If you don't see a command prompt, try pressing enter. ;; connection timed out; no servers could be reached</p> <p>pod \"dns-test-pod\" deleted pod vault-test/dns-test-pod terminated (Error) <pre><code>After adding the correct label:\n</code></pre> k run -it dns-test-pod --image=busybox:1.36.1 --restart=Never --rm --namespace=vault-test -- sh -c \"nslookup kubernetes.default.svc.cluster.local\" Server:     198.19.0.10 Address:    198.19.0.10:53</p> <p>Name:   kubernetes.default.svc.cluster.local Address: 198.19.0.1</p> <p>pod \"dns-test-pod\" deleted <pre><code>In the end I have used this cillium network policy:\n</code></pre> apiVersion: \"cilium.io/v2\" kind: CiliumNetworkPolicy metadata:   name: aws-load-balancer-controller-access   namespace: aws-load-balancer-controller spec:   endpointSelector:     matchLabels: {} # all pods in aws-load-balancer-controller namespace   egress:   # Could use is but I'm using the toEndpoints option to target all Pods in kube-system namespace   # - toEntities:   #   - kube-apiserver   - toFQDNs:     - matchPattern: \".amazonaws.com\"   - toCIDR:     - 169.254.169.254/32  # IMDS endpoint   - toEndpoints:     - matchLabels:         kubernetes.io/metadata.name: kube-system # Allow access to api server, dns server etc.   - toPorts:     # DNS workaround     - ports:         - port: \"53\"           protocol: ANY       rules:         dns:           - matchPattern: \"\"     - ports:         - port: \"443\"           protocol: TCP   ingress:   - fromEntities:     - kube-apiserver # This built-in entity refers to the Kubernetes API server ```</p>","tags":["k8s","cillium","network-policy"]},{"location":"insights/2025/24-10-2025/vault-aws-auth-HCSEC-2025-30/","title":"Vault AWS Auth Method Security Advisory (HCSEC-2025-30)","text":"","tags":["aws","vault"]},{"location":"insights/2025/24-10-2025/vault-aws-auth-HCSEC-2025-30/#summary","title":"Summary","text":"<p>Vault\u2019s STS verification step succeeded (STS proves who signed the request), but Vault\u2019s subsequent authorization/matching logic failed to enforce the account/ARN constraints strictly in some wildcard scenarios, allowing a different-account role with a matching role-name pattern to be accepted.</p>","tags":["aws","vault"]},{"location":"insights/2025/24-10-2025/vault-aws-auth-HCSEC-2025-30/#attack-scenario","title":"Attack Scenario","text":"<ol> <li>An attacker configures an IAM role in their account.</li> <li>The attacker signs a <code>GetCallerIdentity</code> request with that role\u2019s credentials.</li> <li>The attacker sends the signed request to the victim Vault\u2019s AWS Auth endpoint.</li> <li>Vault forwards/verifies the signed request with AWS STS. As far as Authentication is concerned, it passes.</li> <li>AWS STS validates the signature and returns the canonical caller identity for the attacker\u2019s role (so STS \u201cauthenticates\u201d the request).</li> <li>Vault receives a valid STS response and \u2014 due to the vulnerable matching logic for bound_principal_iam/wildcard patterns \u2014 incorrectly authorizes the attacker. So it fails Authorisation.</li> </ol>","tags":["aws","vault"]},{"location":"insights/2025/26-05-2025/scheduling-daemonsets/","title":"Scheduling DaemonSets","text":"<p>Today I had to install the aws-efs-csi-driver and I was faced with the decision of which namespace it should belong to and if I should be adding tolerations to  place it onto certain nodes or all nodes. </p> <p>The default values.yaml file in the helm chart had:</p> <pre><code>  nodeSelector: {}\n  tolerations:\n    - operator: Exists\n</code></pre> <p>To make sense of this:</p> <p>DaemonSets work with taints and tolerations as follows: If a node has a taint, only Pods that tolerate that taint can be scheduled onto it. By default, DaemonSet Pods are not scheduled on tainted nodes (such as control plane nodes) unless you explicitly add tolerations to the Pod template in the DaemonSet. For example, the kube-proxy DaemonSet uses a toleration with <code>operator: Exists</code>, which allows its Pods to tolerate all taints and thus be scheduled on all nodes, including those with taints. You add tolerations in the spec.template.spec.tolerations field of the DaemonSet manifest. This is necessary if you want your DaemonSet Pods to run on nodes with special taints, such as control plane node.</p> <p>So the default which I assume are sensible defaults expect the DaemonSet Pods to run on all nodes.</p> <p>But then I also see nodeSelector and affinitiy options. Now the way I understand it is that Taints and Tolerations are to determine if a Pod can be scheduled onto a Node, NodeSelector is for a preferred node but Affinitiy was unclear to me.</p> <pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: example-daemon\nspec:\n  selector:\n    matchLabels:\n      app: example-daemon\n  template:\n    metadata:\n      labels:\n        app: example-daemon\n    spec:\n      # Tolerations: allow scheduling on tainted nodes (e.g., control-plane)\n      tolerations:\n      - operator: Exists\n      # NodeSelector: restrict to nodes with a specific label\n      nodeSelector:\n        gpu: cuda\n      # Affinity: (added by DaemonSet controller) ensures each Pod is scheduled to a specific node\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchFields:\n              - key: metadata.name\n                operator: In\n                values:\n                - kind-worker\n      containers:\n      - name: example\n        image: busybox\n        command: [\"sleep\", \"infinity\"]\n</code></pre> <p>What this means is:</p> Feature Type Scheduling Effect <code>nodeSelector</code> \ud83d\udd12 Hard Requirement Pod must match specified node labels <code>nodeAffinity</code> (required) \ud83d\udd12 Hard Requirement Like <code>nodeSelector</code>, but more flexible because you can have OR conditions <code>nodeAffinity</code> (preferred) \ud83c\udfaf Soft Preference Try to match, but not required <code>taints</code> + <code>tolerations</code> \ud83d\udd10 Permission System Node repels pods unless tolerated <p>In other words you can achieve the same thing as <code>NodeSelector</code> with <code>Affinity</code>. However, <code>NodeSelector</code> is more simple and direct unless you want to match multiple labels, then you need <code>Affinity</code>.</p> <p>A side on on Affinity's <code>nodeAffinitiy.requiredDuringSchedulingIgnoredDuringExecution</code></p> <p>It has two parts:</p> <ol> <li> <p><code>requiredDuringScheduling</code>: This means the condition must be met at the time the Pod is scheduled. If no matching node is found, the Pod will not be scheduled at all \u2014 just like a nodeSelector.</p> </li> <li> <p><code>IgnoredDuringExecution</code>: This means once the Pod is running, Kubernetes won\u2019t evict it even if the node\u2019s labels change and no longer match the rule.</p> </li> </ol>","tags":["k8s","daemonsets"]},{"location":"notes/aws-faq/","title":"AWS FAQ","text":"","tags":["aws"]},{"location":"notes/aws-faq/#why-does-elb-have-its-own-account-ids","title":"Why does ELB have its own account IDs?","text":"<p>https://stackoverflow.com/questions/76974503/why-does-elb-have-its-own-account-ids</p>","tags":["aws"]},{"location":"notes/vault-412-index-not-present/","title":"Vault 412 Index Not Present","text":"","tags":["vault"]},{"location":"notes/vault-412-index-not-present/#whats-the-problem","title":"What's the problem?","text":"<p>The following error is returned when trying to retrieve a secret via a GitLab pipeline:</p> <p>412 required index state not present</p>","tags":["vault"]},{"location":"notes/vault-412-index-not-present/#what-do-we-know-so-far","title":"What do we know so far?","text":"<p>Based on preliminary observation it appears that:</p> <ol> <li>When a secret is added or updated or even a configuration change is made and the secret is retrieved via the pipeline, the 412 error is intermittently returned</li> <li>The error can return even after approximately 2mins of adding/updating the secret</li> <li>Reproducible in dev and prod</li> </ol>","tags":["vault"]},{"location":"notes/vault-412-index-not-present/#what-can-we-do-to-isolate-the-problem-further","title":"What can we do to isolate the problem further?","text":"<ol> <li>Attempt to set <code>VAULT_MAX_RETRIES</code> to a very high threshold <ul> <li>Tested adding a secret and running the pipeline immediately which failed. On subsequent re-runs of the pipeline it worked. </li> </ul> </li> <li>Retrieve the GitLab ID token, login locally and attempt to retrieve the secret in a loop to identify failure rate<ul> <li>Can confirm that running a curl command occasionally returns a <code>index not present</code> error  </li> </ul> </li> <li>Ensure the clock time is in sync</li> <li>Identify if there are any latency metrics we can look at</li> <li>Enable trace logs </li> </ol>","tags":["vault"]},{"location":"notes/vault-412-index-not-present/#what-is-known-about-this-behaviour","title":"What is known about this behaviour?","text":"<p>The  Server Side Consistent Tokens feature introduced in v1.10 embeds the WAL index in the token to allow performance standby nodes to check the token locally against its index. If  the request made to the node cannot support read-after-write consistency because WAL index for  the token does not match, it will result in a 412 index not present.</p> <p>The key point is:</p> <p>Unless there is a considerable replication delay, Vault clients experience read-after-write consistency.</p> <p>In our case given that the issue appears to surface even after ~2mins, it is likely there is an actual  replication delay. Either that or there are genuine changes occurring causing the WAL to change and the client is not retrying.</p>","tags":["vault"]},{"location":"notes/vim-shortcuts/","title":"Vim shortcuts","text":"<p>\ud83e\udded Vim Navigation Shortcuts Cheat Sheet</p> <p>\ud83d\udd3c Basic Movement | Shortcut | Description                            | Example / Notes       | | -------- | -------------------------------------- | --------------------- | | <code>h</code>      | Move left                              |                       | | <code>l</code>      | Move right                             |                       | | <code>j</code>      | Move down                              |                       | | <code>k</code>      | Move up                                |                       | | <code>0</code>      | Move to beginning of line              |                       | | <code>^</code>      | Move to first non-whitespace character |                       | | <code>$</code>      | Move to end of line                    |                       | | <code>gg</code>     | Go to beginning of file                |                       | | <code>G</code>      | Go to end of file                      |                       | | <code>:n</code>     | Go to line <code>n</code>                         | <code>:42</code> goes to line 42 | | <code>zz</code>     | Center current line in window          | Great for readability |</p> <p>\ud83d\udce6 Word/Block Movement | Shortcut | Description                          | Example / Notes        | | -------- | ------------------------------------ | ---------------------- | | <code>w</code>      | Move to beginning of next word       |                        | | <code>e</code>      | Move to end of current word          |                        | | <code>b</code>      | Move to beginning of previous word   |                        | | <code>ge</code>     | Move to end of previous word         |                        | | <code>%</code>      | Jump to matching bracket/parenthesis | Useful for code blocks |</p> <p>\ud83d\udd0d Search &amp; Navigate | Shortcut   | Description                           | Example / Notes | | ---------- | ------------------------------------- | --------------- | | <code>/pattern</code> | Search forward for <code>pattern</code>          | <code>/AuthMethod</code>   | | <code>?pattern</code> | Search backward                       |                 | | <code>n</code>        | Repeat last search forward            |                 | | <code>N</code>        | Repeat last search backward           |                 | | <code>*</code>        | Search for word under cursor forward  |                 | | <code>#</code>        | Search for word under cursor backward |                 |</p> <p>\ud83d\ude80 Advanced Navigation | Shortcut            | Description                        | Example / Notes             | | ------------------- | ---------------------------------- | --------------------------- | | <code>''</code> or ````    | Return to previous cursor position | After jumping around        | | <code>Ctrl + o</code>          | Go to older cursor position        | Like a back button          | | <code>Ctrl + i</code>          | Go to newer cursor position        | Like a forward button       | | <code>H</code>                 | Move to top of screen              |                             | | <code>M</code>                 | Move to middle of screen           |                             | | <code>L</code>                 | Move to bottom of screen           |                             | | <code>:bnext</code> / <code>:bn</code>    | Next buffer                        | When editing multiple files | | <code>:bprev</code> / <code>:bp</code>    | Previous buffer                    |                             | | <code>:ls</code> or <code>:buffers</code> | List open buffers                  |                             |</p> <p>\ud83d\udcc1 File Navigation | Shortcut            | Description                   | Example / Notes                              | | ------------------- | ----------------------------- | -------------------------------------------- | | <code>:e &lt;file&gt;</code>         | Open a file                   | <code>:e main.go</code>                                 | | <code>:Ex</code> or <code>:Explore</code> | Open file browser (netrw)     | Navigate folders inside Vim                  | | <code>gf</code>                | Go to file under cursor       | Opens the file if it exists                  | | <code>:vsplit &lt;file&gt;</code>    | Open file in vertical split   | Side-by-side editing                         | | <code>:split &lt;file&gt;</code>     | Open file in horizontal split |                                              | | <code>Ctrl + w, h/j/k/l</code> | Navigate between splits       | Like Vim\u2019s version of arrow keys for windows |</p>"},{"location":"notes/vim-shortcuts/#selecting-a-function-block","title":"Selecting a Function block","text":"<p>\ud83e\udde0 Explanation</p> <p><code>va{</code>: selects the function block in characterwise visual mode.</p> <p><code>V</code>  : changes it to a linewise selection.</p> <p><code>O</code>  : switches the active end of the visual selection to the top.</p> <p><code>k</code>  : moves it up, including the comment line above the function.</p> <p>Use the following to select the funtion comment for example via <code>O</code>. It also shows navigation options  surrounding the function.</p> Action Keys Effect Extend selection up <code>O</code> + <code>k</code> Moves the start of the selection up Extend selection down <code>o</code> + <code>j</code> Moves the end of the selection down"},{"location":"retrospectives/2023/17-04-2023/","title":"17 04 2023","text":"","tags":["dont-repeat"]},{"location":"retrospectives/2023/17-04-2023/#retrospective","title":"Retrospective","text":"<ol> <li> <p>Git recursive error when setting up <code>.gitconfig</code> file <code>includeIf</code></p> <p>Failed to noticed how I was including the same file within the target file created.</p> </li> <li> <p>Bash error <code>unexpected EOF while looking for matching \"'</code></p> <p>I was only looking at the current line instead of the surrounding code I had previously added.  Missed closing quote for a variable that previously defined.</p> </li> </ol>","tags":["dont-repeat"]},{"location":"retrospectives/2023/19-07-2023/","title":"19 07 2023","text":"","tags":["git"]},{"location":"retrospectives/2023/19-07-2023/#retrospective","title":"Retrospective","text":"<ol> <li>Git committer email mismatch</li> </ol> <p>While trying to push to a development instance of GitLab, I encountered the following error:</p> <pre><code>remote: GitLab: Committer's email 'John.Doe@team.example.com' does not follow the pattern '@test\\.team\\.example\\.com$'\n</code></pre> <p>I had the correct email configured in my local and global <code>~/.gitconfig</code> file with multiple git profiles to target different environments.</p> <p>Running <code>git config user.email</code> within the repo displayed the correct email as per regex pattern. </p> <p>After some head scratching it occured to me that the initial commit was using <code>John.Doe@team.example.com</code> (I only setup the git profile for dev after the first commit!).</p> <p>Had to fix this by running:</p> <pre><code>git -c user.name=\"John Doe\" -c user.email=John.Doe@test.team.example.com commit --amend --reset-author\n</code></pre> <p>This will amend the last commit, resetting the author email to match the required pattern.</p>","tags":["git"]},{"location":"retrospectives/2023/21-04-2023/","title":"21 04 2023","text":"","tags":["dont-repeat"]},{"location":"retrospectives/2023/21-04-2023/#retrospective","title":"Retrospective","text":"","tags":["dont-repeat"]},{"location":"retrospectives/2023/21-04-2023/#stick-to-the-plan","title":"Stick to the plan","text":"<p>I've been trying to develop a programming  design flow that allows me to keep the bigger picture in mind while working on the tasks at hand. My current flow goes something like this once the requirements are clear:</p> <ol> <li>Create a skeleton of the codebase to see the end to end flow. This is meant to enable visualising the call flow, dependencies    etc. Avoid writing any implementation details. Focus on stringing things together to see enough of the program flow, specially    from a developer experience perspective e.g: How easy is to make an API call? </li> <li>Refine the APIs of the codebase to reflect the expected API calls you intend to make and use.</li> <li>Rinse and repeat until you have clear APIs.</li> </ol>","tags":["dont-repeat"]},{"location":"retrospectives/2023/21-04-2023/#where-i-went-wrong","title":"Where I went wrong","text":"<p>I faltered when I got caught up in wanting to get things to work and delving into the implementation details immediately instead of following the plan. The problem here is once you are focused on just making things work, the code design mindset moves out of your primary focus. Sure, you can come back and refactor but at times, there is a chain reaction of refactoring that could have been avoided. I'd say more times than not, this is what has happened to me.</p> <p>I also made the cardinal sin of not adding git commits because I had gone too far down the rabbit hole trying to make things work and left with a whale of diffs to categorically pick and commit.</p>","tags":["dont-repeat"]},{"location":"retrospectives/2025/25-06-2025/","title":"AWS CloudWatch","text":"<p>https://aws.amazon.com/blogs/mt/analyzing-aws-waf-logs-in-amazon-cloudwatch-logs/</p>","tags":["cloudwatch"]},{"location":"retrospectives/2025/alb-cilium-networking/","title":"ALB and Cilium","text":"<p>Applied the AWS LB controller Ingress in the hope that it will just work but I turns out while the TargetGroup healthcheck to the control plane nodes was successful, the healthchecks were failing on the worker nodes. The existing Cilium network policy allowed access from/to k8s API server only.</p> <p>Ok if I try to think through this, the first question I need to ask is what is different about the control plane nodes vs worker nodes?</p>","tags":["cilium"]},{"location":"still-cooking/","title":"About","text":"<p>Sometimes notes don't get finished, refined, or even properly started\u2014but they still deserve a home. One day, I may return to this little fragment of thought and turn it into something coherent. Until then, it proudly stands here, half-baked and hopeful.</p>"},{"location":"still-cooking/aws-waf-alb-retro/","title":"About","text":"<p>Captures my thought process to illuminate reasoning.</p>","tags":["alb","waf"]},{"location":"still-cooking/aws-waf-alb-retro/#alb-logs","title":"ALB Logs","text":"<p>ALB provides 2 types of logs; access logs and connection logs. While the access logs are self explanatory and should be be enabled, the connection logs appear to provide value in inspecting TLS connection details. These logs while useful for diagnosing connection issues, it may not provide value in terms of a cost benefit to capture all the logs for every connection. The security baseline also makes no mention of connection logs. Therefore it may only be required to be enabled when connection issues are encountered.</p>","tags":["alb","waf"]},{"location":"still-cooking/bash-primer/","title":"Bash Primer","text":"","tags":["bash"]},{"location":"still-cooking/bash-primer/#finding-a-command","title":"Finding a command","text":"<p>The bash builtin type command searches your environment (including aliases, keywords, functions, builtins, directories in $PATH, and the command hash table) for executable commands unlike the <code>which</code> command:</p> <pre><code>$ type ls -a\nls is an alias for ls -G\nls is /bin/ls\n</code></pre>","tags":["bash"]},{"location":"still-cooking/bash-primer/#cant-remember-the-exact-command-name","title":"Can't remember the exact command name","text":"<pre><code>man -k movie\n</code></pre>","tags":["bash"]},{"location":"still-cooking/bash-primer/#find-files","title":"Find files","text":"<p>locate and slocate consult database files about the system  <pre><code>$ locate ls\n\n# Only show programs the user has access to\n$ slocate ls\n</code></pre></p>","tags":["bash"]},{"location":"still-cooking/bash-primer/#find-information-about-a-file","title":"Find information about a file","text":"<pre><code>$ stat test.log\n  File: test.log\n  Size: 6950        Blocks: 16         IO Block: 4096   regular file\nDevice: 10301h/66305d   Inode: 259552      Links: 1\nAccess: (0660/-rw-rw----)  Uid: ( 1000/  ubuntu)   Gid: ( 1000/  ubuntu)\nAccess: 2024-05-03 06:25:18.293530773 +1000\nModify: 2024-01-31 18:35:17.855147535 +1100\nChange: 2024-01-31 18:35:17.855147535 +1100\n Birth: 2024-01-31 18:15:34.763925028 +1100\n</code></pre>","tags":["bash"]},{"location":"still-cooking/bash-primer/#show-only-dot-files-in-a-directory","title":"Show only dot files in a directory","text":"<p>Every normal directory contains a <code>.</code> and <code>..</code>. Ignore this with <code>la -A</code>.</p> <pre><code>shows contents of the .ansible directory including . and ..\n$ ls -a .ansible \n\n\n$ ls -d .ansible\n.ansible\n</code></pre> <pre><code>$ ls -d .*\n$ ls -d .b*\n</code></pre>","tags":["bash"]},{"location":"still-cooking/bash-primer/#search-positive-look-ahead","title":"Search: Positive look ahead","text":"<p>Say you have the following content in a file:</p> <pre><code>## Heading A\n\n### Heading B\n</code></pre> <p>Now to change <code>##</code> to <code>###</code> use a lookahead for the match but not include it in the search result because you don't want include the empty char in the search that follows from the <code>#</code> to heading string.</p> <pre><code>(?=) : postivie lookahead\n^##(?=\\s)\n</code></pre>","tags":["bash"]},{"location":"still-cooking/ssl-tcpdump/","title":"Ssl tcpdump","text":"<pre><code> sudo tcpdump 'tcp[tcpflags] &amp; (tcp-rst|tcp-syn|tcp-fin) != 0  and host $HOST_NAME' -i any\n</code></pre> <pre><code> 00:00:00.000000 ens5  Out IP 10.61.69.173.50520 &gt; 10.219.208.6.443: Flags [S], seq 358560042, win 62727, options [mss 8961,sackOK,TS val 4176215147 ecr 0,nop,wscale 7], length 0\n 00:00:00.008208 ens5  In  IP 10.219.208.6.443 &gt; 10.61.69.173.50520: Flags [S.], seq 897476341, ack 358560043, win 64240, options [mss 1375,nop,nop,sackOK,nop,wscale 7], length 0\n 00:00:00.000046 ens5  Out IP 10.61.69.173.50520 &gt; 10.219.208.6.443: Flags [.], ack 897476342, win 491, length 0\n 00:00:00.000921 ens5  Out IP 10.61.69.173.50520 &gt; 10.219.208.6.443: Flags [P.], seq 358560043:358560376, ack 897476342, win 491, length 333\n 00:00:00.007416 ens5  In  IP 10.219.208.6.443 &gt; 10.61.69.173.50520: Flags [.], ack 358560376, win 501, length 0\n 00:00:00.000708 ens5  In  IP 10.219.208.6.443 &gt; 10.61.69.173.50520: Flags [F.], seq 897476342, ack 358560376, win 501, length 0\n 00:00:00.000085 ens5  Out IP 10.61.69.173.50520 &gt; 10.219.208.6.443: Flags [P.], seq 358560376:358560383, ack 897476343, win 491, length 7\n 00:00:00.000126 ens5  Out IP 10.61.69.173.50520 &gt; 10.219.208.6.443: Flags [F.], seq 358560383, ack 897476343, win 491, length 0\n 00:00:00.008942 ens5  In  IP 10.219.208.6.443 &gt; 10.61.69.173.50520: Flags [R], seq 897476343, win 0, length 0\n</code></pre>"},{"location":"still-cooking/vault-aws-se.mm/","title":"Vault AWS SE","text":"<p>This is a collection of resources to skill up in Cloud and Backend Engineering.</p> <p>Note</p> <p>This list is a work in progress.</p>"},{"location":"still-cooking/vault-aws-se.mm/#iam-user","title":"IAM User","text":""},{"location":"still-cooking/vault-aws-se.mm/#result","title":"Result","text":"<ul> <li>access key ID and secret access key</li> </ul>"},{"location":"still-cooking/vault-aws-se.mm/#expiry","title":"Expiry","text":"<ul> <li>Lease applies if configured and will expire</li> </ul>"},{"location":"still-cooking/vault-aws-se.mm/#config","title":"Config","text":"<ul> <li>IAM user creds </li> <li>Dynamically creates users </li> </ul>"},{"location":"still-cooking/vault-aws-se.mm/#expiry_1","title":"Expiry","text":"<ul> <li>The lease is set at the root config level</li> <li>Users are deleted at the end of the lease</li> </ul>"},{"location":"still-cooking/vault-aws-se.mm/#federation-token","title":"Federation Token","text":""},{"location":"still-cooking/vault-aws-se.mm/#result_1","title":"Result","text":"<ul> <li>Temporary access key ID, a secret access key, and a security token</li> </ul>"},{"location":"still-cooking/vault-aws-se.mm/#config_1","title":"Config","text":"<ul> <li>You must call the API operation using long-term creds of an IAM user</li> </ul>"},{"location":"still-cooking/vault-aws-se.mm/#expiry_2","title":"Expiry","text":"<ul> <li>Can be set at the role configuration </li> </ul>"},{"location":"still-cooking/vault-aws-se.mm/#use-case","title":"Use case","text":"<ul> <li>Proxy application that gets creds on behalf of distributed apps inside a coporate network</li> </ul>"},{"location":"still-cooking/vault-aws-se.mm/#limitations","title":"Limitations","text":"<ul> <li>Cannot call IAM API</li> <li>Can only call <code>GetCallerIdentity</code> of STS API</li> <li>Must pass in-line or managed policy </li> <li>Intersection of the IAM user policy and session policy which can be furter restricted </li> <li>Max permissions are limited to the IAM user policy </li> </ul>"},{"location":"still-cooking/vault-aws-se.mm/#sts-session-tokens","title":"STS Session Tokens","text":""},{"location":"still-cooking/vault-aws-se.mm/#result_2","title":"Result","text":"<ul> <li>Temporary access key ID, a secret access key, and a security token</li> </ul>"},{"location":"still-cooking/vault-aws-se.mm/#config_2","title":"Config","text":"<ul> <li>You must call the API operation using long-term creds of an IAM user</li> </ul>"},{"location":"still-cooking/vault-aws-se.mm/#expiry_3","title":"Expiry","text":"<ul> <li>Can be set at the role configuration </li> </ul>"},{"location":"still-cooking/vault-aws-se.mm/#use-case_1","title":"Use case","text":"<ul> <li>Session tokens may also require an MFA-based TOTP to be provided if the IAM user is configured to require it</li> </ul>"},{"location":"still-cooking/vault-aws-se.mm/#limitations_1","title":"Limitations","text":"<ul> <li>Inherits all or any permissions of the IAM user of root config </li> <li>assigning a role or policy is disallowed </li> <li>IAM API can only if MFA details are included</li> </ul>"},{"location":"still-cooking/vault-aws-se.mm/#differences","title":"Differences","text":""},{"location":"still-cooking/vault-aws-se.mm/#sts-session-vs-ferderated-token","title":"STS Session vs. Ferderated token","text":""},{"location":"still-cooking/vault-aws-se.mm/#federated-tokens","title":"Federated tokens","text":"<ul> <li>Can be configured with additional policies or roles</li> <li>IAM API cannot be called</li> </ul>"},{"location":"still-cooking/vault-aws-se.mm/#sts-session-tokens_1","title":"STS session tokens","text":"<ul> <li>Inherits the root config IAM user policy. No further policies can be attached.</li> </ul>"},{"location":"still-cooking/vault-aws-se.mm/#assume-role","title":"Assume role","text":"<ul> <li>Can invoke STS and IAM API if the assume role's policy grants it </li> <li>Cross account authentication </li> </ul>"},{"location":"still-cooking/vault-aws-se.mm/#references","title":"References","text":"<ul> <li>Request temp security Credentials</li> </ul>"},{"location":"still-cooking/vault-aws-se.mm/#root-config","title":"Root config","text":"<ul> <li>Must contain IAM user creds </li> <li>Should run the rotate command so Vault auto rotates the creds </li> <li> </li> </ul>"},{"location":"still-cooking/vault-aws-se.mm/#all-of-the-role-types-depend-on-the-root-config-iam-user","title":"All of the role types depend on the root config IAM user","text":""},{"location":"still-cooking/vault-k8s-auth/","title":"Vault K8s Auth","text":"","tags":["vault","k8s"]},{"location":"still-cooking/vault-k8s-auth/#when-you-create-a-serviceaccount-does-it-automatically-create-a-token","title":"When you create a ServiceAccount does it automatically create a token?","text":"","tags":["vault","k8s"]},{"location":"still-cooking/vault-k8s-auth/#what-is-the-tokenreview-api-return-values","title":"What is the <code>TokenReview API</code> return values?","text":"","tags":["vault","k8s"]},{"location":"still-cooking/vault-k8s-auth/#how-is-the-aud-claim-used-for-sa-tokens-and-vault","title":"How is the <code>aud</code> claim used for SA tokens and Vault?","text":"","tags":["vault","k8s"]},{"location":"still-cooking/vault-k8s-auth/#what-are-the-configuration-details-for-authjwtconfig-api","title":"What are the configuration details for auth/jwt/config API?","text":"","tags":["vault","k8s"]},{"location":"still-cooking/vault-k8s-auth/#what-is-the-threat-model","title":"What is the threat model?","text":"<p>https://developer.hashicorp.com/vault/docs/auth/kubernetes</p> <p>The pattern Vault uses to authenticate Pods depends on sharing the JWT token over the network. Given the security model of Vault, this is allowable because Vault is part of the trusted compute base. In general, Kubernetes applications should not share this JWT with other applications, as it allows API calls to be made on behalf of the Pod and can result in unintended access being granted to 3rd parties.</p>","tags":["vault","k8s"]},{"location":"still-cooking/vault-k8s-auth/#what-is-the-authentication-method","title":"What is the Authentication method?","text":"<p>https://developer.hashicorp.com/vault/docs/auth/jwt/oidc-providers/kubernetes#use-service-account-issuer-discovery</p> <p>Service Account issuer discovery as per:</p> <p>https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-issuer-discovery</p> <ul> <li>Endpoint must be unauthenticated to federate with Vault</li> <li><code>oidc_discovery_url</code> and if custom CA certs are used <code>oidc_discovery_ca_pem</code> must be provided</li> <li></li> </ul>","tags":["vault","k8s"]},{"location":"still-cooking/vault-k8s-auth/#what-are-the-token-security-features","title":"What are the token security features?","text":"","tags":["vault","k8s"]},{"location":"still-cooking/vault-k8s-auth/#what-course-of-action-to-take-if-the-oidc-discovery-url-is-not-public","title":"What course of action to take if the OIDC discovery URL is not public?","text":"","tags":["vault","k8s"]},{"location":"still-cooking/vault-k8s-auth/#why-should-the-iss-claim-be-validated-against-the-issuer-url","title":"Why should the <code>iss</code> claim be validated against the issuer URL?","text":"","tags":["vault","k8s"]},{"location":"still-cooking/vault-k8s-auth/#what-is-the-difference-between-rsa-or-ecdsa-certificate","title":"What is the difference between <code>RSA</code> or <code>ECDSA</code> certificate?","text":"<p>https://go.dev/blog/tls-cipher-suites</p>","tags":["vault","k8s"]},{"location":"still-cooking/vault-k8s-auth/#what-is-the-purpose-of-a-ciphersuite","title":"What is the purpose of a ciphersuite?","text":"","tags":["vault","k8s"]},{"location":"still-cooking/vault-k8s-auth/#confidentiality-vs-integrity","title":"Confidentiality vs. Integrity","text":"<p>onfidentiality is provided via encryption, while Integrity is provided via MAC.</p>","tags":["vault","k8s"]},{"location":"still-cooking/vault-k8s-auth/#public-key-trust","title":"Public key trust","text":"<p>You should be able to trust the public key this is why certificate verification is important.</p> <p>As per the spec:</p> <p>issuer URL == issuer value  issue claim == issuer value</p>","tags":["vault","k8s"]},{"location":"still-cooking/vault-k8s-auth/#why-is-aud-claim-important","title":"Why is <code>aud</code> claim important?","text":"","tags":["vault","k8s"]},{"location":"still-cooking/vault-k8s-auth/#why-is-sub-claim-validation-important","title":"Why is <code>sub</code> claim validation important?","text":"<ul> <li>Avoid misuse</li> </ul> <p>Say a token has subject <code>app-service-account-1</code> and someone attempt to access a resource meant for <code>app-service-account-2</code>, even   though the signature validation will pass, the <code>sub</code> validation will fail. This prevents misuse of a valid token.</p> <ul> <li>Token replay    If an attacker steals a token with subject <code>app-service-account-1</code> and attempts to access resources for <code>app-service-account-2</code>, even though signature verification will pass, bound claim validation will fail. </li> </ul>","tags":["vault","k8s"]},{"location":"still-cooking/vault-k8s-auth/#why-is-sub-claim-validation-required-if-signature-verification-takes-place","title":"Why is <code>sub</code> claim validation required if signature verification takes place?","text":"<p>Signature verification provides an integrity check while the subject verifies the entity. This ensure only specific SAs can access specific roles.</p>","tags":["vault","k8s"]},{"location":"still-cooking/vault-k8s-auth/#commands","title":"Commands","text":"<ol> <li>How to find issuer?</li> </ol> <p>kubectl get --raw /.well-known/openid-configuration | jq -r '.issuer'</p>","tags":["vault","k8s"]}]}